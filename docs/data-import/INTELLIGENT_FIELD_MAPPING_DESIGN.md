# æ™ºèƒ½å­—æ®µæ˜ å°„è®¾è®¡æ–‡æ¡£

**åˆ›å»ºæ—¶é—´**: 2025-01-23  
**ç‰ˆæœ¬**: 1.0  
**çŠ¶æ€**: âœ… **è®¾è®¡å®Œæˆï¼Œå¾…å®ç°**

---

## ğŸ“‹ æ ¸å¿ƒéœ€æ±‚

åœ¨æ•°æ®å¯¼å…¥æ—¶ï¼Œç”¨æˆ·æä¾›çš„æºæ–‡ä»¶æ ¼å¼ã€å­—æ®µåç§°ä¼šå‡ºç°å˜åŒ–ï¼Œä½†æœ‰ä¸€å®šç¨³å®šæ€§ã€‚å› æ­¤éœ€è¦ï¼š

1. âœ… **æ™ºèƒ½æ˜ å°„**ï¼šè‡ªåŠ¨æ¨èå­—æ®µæ˜ å°„å…³ç³»ï¼Œé™ä½ç”¨æˆ·æ“ä½œéš¾åº¦
2. âœ… **å†å²è®°å¿†**ï¼šè®°å½•å†å²æ˜ å°„è®°å½•ï¼Œä½œä¸ºä¸‹æ¬¡æ™ºèƒ½æ˜ å°„çš„å‚è€ƒ
3. âœ… **æŒç»­å­¦ä¹ **ï¼šç”¨æˆ·ç¡®è®¤çš„æ˜ å°„ä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œæé«˜åç»­æ¨èçš„å‡†ç¡®æ€§
4. âœ… **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šè€ƒè™‘æ•°æ®æºç±»å‹ã€å•æ®ç±»å‹ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯

---

## ğŸ¯ ç³»ç»Ÿæ¶æ„

### æ•°æ®æµç¨‹

```
1. ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
   â†“
2. ç³»ç»Ÿè§£ææ–‡ä»¶å­—æ®µ
   â†“
3. æŸ¥è¯¢å†å²æ˜ å°„è®°å½•ï¼ˆåŸºäºï¼šæ•°æ®æºç±»å‹ã€å•æ®ç±»å‹ã€ç”¨æˆ·IDï¼‰
   â†“
4. æ™ºèƒ½æ˜ å°„ç®—æ³•
   â”œâ”€â”€ å†å²æ˜ å°„åŒ¹é…ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
   â”œâ”€â”€ ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ã€è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
   â””â”€â”€ ä¸Šä¸‹æ–‡è§„åˆ™ï¼ˆæ•°æ®æºç‰¹å®šçš„æ˜ å°„è§„åˆ™ï¼‰
   â†“
5. ç”Ÿæˆæ˜ å°„æ¨èåˆ—è¡¨
   â”œâ”€â”€ é«˜ç½®ä¿¡åº¦æ˜ å°„ï¼ˆè‡ªåŠ¨åº”ç”¨ï¼‰
   â”œâ”€â”€ ä¸­ç½®ä¿¡åº¦æ˜ å°„ï¼ˆæ¨èç»™ç”¨æˆ·ç¡®è®¤ï¼‰
   â””â”€â”€ ä½ç½®ä¿¡åº¦æ˜ å°„ï¼ˆæç¤ºç”¨æˆ·ï¼‰
   â†“
6. ç”¨æˆ·ç¡®è®¤æˆ–è°ƒæ•´æ˜ å°„
   â†“
7. è®°å½•ç¡®è®¤åçš„æ˜ å°„åˆ°å†å²è®°å½•
   â†“
8. åº”ç”¨æ˜ å°„ï¼Œç»§ç»­å¤„ç†
```

---

## ğŸ“Š æ•°æ®åº“è®¾è®¡

### 1. å­—æ®µæ˜ å°„å†å²è¡¨

```sql
-- å­—æ®µæ˜ å°„å†å²è®°å½•è¡¨
CREATE TABLE field_mapping_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- ä¸Šä¸‹æ–‡ä¿¡æ¯
    source_system VARCHAR(50) NOT NULL,        -- æ•°æ®æºç³»ç»Ÿï¼ˆå¦‚ï¼šerp_system_a, manual, ...ï¼‰
    document_type VARCHAR(50),                  -- å•æ®ç±»å‹ï¼ˆå¦‚ï¼špurchase_order, sales_order, ...ï¼‰
    user_id UUID,                               -- ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œä¸ªäººåŒ–æ˜ å°„ï¼‰
    
    -- æºå­—æ®µä¿¡æ¯
    source_field_name VARCHAR(255) NOT NULL,    -- æºæ–‡ä»¶å­—æ®µå
    source_field_type VARCHAR(50),              -- æºå­—æ®µç±»å‹ï¼ˆstring, number, date, ...ï¼‰
    source_field_position INTEGER,              -- å­—æ®µä½ç½®ï¼ˆå¦‚æœæ²¡æœ‰åˆ—åï¼Œä½¿ç”¨ä½ç½®ï¼‰
    
    -- ç›®æ ‡å­—æ®µä¿¡æ¯
    target_field_name VARCHAR(255) NOT NULL,    -- ç›®æ ‡æ ‡å‡†å­—æ®µå
    target_field_category VARCHAR(50),          -- å­—æ®µç±»åˆ«ï¼ˆheader, detail, amount, ...ï¼‰
    
    -- åŒ¹é…ä¿¡æ¯
    match_confidence DECIMAL(5,2),             -- åŒ¹é…ç½®ä¿¡åº¦ï¼ˆ0-100ï¼‰
    match_method VARCHAR(50),                   -- åŒ¹é…æ–¹æ³•ï¼ˆhistory, similarity, rule, manualï¼‰
    
    -- ä½¿ç”¨ç»Ÿè®¡
    usage_count INTEGER DEFAULT 1,              -- ä½¿ç”¨æ¬¡æ•°
    last_used_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,  -- æœ€åä½¿ç”¨æ—¶é—´
    first_used_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,  -- é¦–æ¬¡ä½¿ç”¨æ—¶é—´
    
    -- ç”¨æˆ·åé¦ˆ
    is_confirmed BOOLEAN DEFAULT TRUE,         -- æ˜¯å¦è¢«ç”¨æˆ·ç¡®è®¤
    is_rejected BOOLEAN DEFAULT FALSE,          -- æ˜¯å¦è¢«ç”¨æˆ·æ‹’ç»
    
    -- å…ƒæ•°æ®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- ç´¢å¼•
    CONSTRAINT unique_mapping UNIQUE (source_system, document_type, source_field_name, target_field_name, user_id)
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_field_mapping_context ON field_mapping_history(source_system, document_type, user_id);
CREATE INDEX idx_field_mapping_source ON field_mapping_history(source_field_name);
CREATE INDEX idx_field_mapping_target ON field_mapping_history(target_field_name);
CREATE INDEX idx_field_mapping_usage ON field_mapping_history(usage_count DESC, last_used_at DESC);
```

### 2. å­—æ®µæ˜ å°„è§„åˆ™è¡¨ï¼ˆå¯é€‰ï¼‰

```sql
-- å­—æ®µæ˜ å°„è§„åˆ™è¡¨ï¼ˆç³»ç»Ÿçº§è§„åˆ™ï¼Œä¸éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰
CREATE TABLE field_mapping_rules (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- è§„åˆ™æ¡ä»¶
    source_system VARCHAR(50),                  -- é€‚ç”¨æ•°æ®æºï¼ˆNULLè¡¨ç¤ºæ‰€æœ‰ï¼‰
    document_type VARCHAR(50),                  -- é€‚ç”¨å•æ®ç±»å‹ï¼ˆNULLè¡¨ç¤ºæ‰€æœ‰ï¼‰
    
    -- åŒ¹é…æ¨¡å¼
    source_pattern VARCHAR(255) NOT NULL,      -- æºå­—æ®µæ¨¡å¼ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    match_type VARCHAR(50) DEFAULT 'exact',     -- åŒ¹é…ç±»å‹ï¼šexact, prefix, suffix, regex, contains
    
    -- ç›®æ ‡å­—æ®µ
    target_field_name VARCHAR(255) NOT NULL,    -- ç›®æ ‡æ ‡å‡†å­—æ®µå
    
    -- ä¼˜å…ˆçº§
    priority INTEGER DEFAULT 100,               -- ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    
    -- çŠ¶æ€
    is_active BOOLEAN DEFAULT TRUE,            -- æ˜¯å¦å¯ç”¨
    
    -- å…ƒæ•°æ®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- ç´¢å¼•
    CREATE INDEX idx_field_mapping_rules_pattern ON field_mapping_rules(source_pattern);
);
```

---

## ğŸ§  æ™ºèƒ½æ˜ å°„ç®—æ³•

### 1. æ˜ å°„æ¨èç®—æ³•

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
from difflib import SequenceMatcher
import sqlalchemy as sa
from sqlalchemy.orm import Session

@dataclass
class MappingCandidate:
    """æ˜ å°„å€™é€‰"""
    target_field: str
    confidence: float  # 0-1
    method: str  # 'history', 'similarity', 'rule', 'manual'
    source: str  # æ¨èæ¥æºæè¿°

@dataclass
class FieldMappingRecommendation:
    """å­—æ®µæ˜ å°„æ¨èç»“æœ"""
    source_field: str
    candidates: List[MappingCandidate]
    recommended_target: Optional[str] = None
    recommended_confidence: float = 0.0

class IntelligentFieldMapper:
    """æ™ºèƒ½å­—æ®µæ˜ å°„å™¨"""
    
    def __init__(self, db_session: Session):
        self.db = db_session
        
    def recommend_mappings(
        self,
        source_fields: List[str],
        source_system: str,
        document_type: Optional[str] = None,
        user_id: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> List[FieldMappingRecommendation]:
        """æ¨èå­—æ®µæ˜ å°„
        
        Args:
            source_fields: æºæ–‡ä»¶å­—æ®µåˆ—è¡¨
            source_system: æ•°æ®æºç³»ç»Ÿ
            document_type: å•æ®ç±»å‹
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºä¸ªäººåŒ–æ¨èï¼‰
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            æ˜ å°„æ¨èåˆ—è¡¨
        """
        recommendations = []
        
        for source_field in source_fields:
            candidates = []
            
            # 1. æŸ¥è¯¢å†å²æ˜ å°„ï¼ˆä¼˜å…ˆï¼‰
            history_candidates = self._get_history_mappings(
                source_field, source_system, document_type, user_id
            )
            candidates.extend(history_candidates)
            
            # 2. åº”ç”¨æ˜ å°„è§„åˆ™
            rule_candidates = self._apply_mapping_rules(
                source_field, source_system, document_type
            )
            candidates.extend(rule_candidates)
            
            # 3. è®¡ç®—ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¦‚æœæ²¡æœ‰å†å²è®°å½•æˆ–è§„åˆ™ï¼‰
            if not candidates or max(c.confidence for c in candidates) < 0.8:
                similarity_candidates = self._calculate_similarity_mappings(
                    source_field, document_type
                )
                candidates.extend(similarity_candidates)
            
            # 4. æ’åºå’Œå»é‡
            candidates = self._deduplicate_and_sort_candidates(candidates)
            
            # 5. æ„å»ºæ¨èç»“æœ
            recommended_target = candidates[0].target_field if candidates else None
            recommended_confidence = candidates[0].confidence if candidates else 0.0
            
            recommendations.append(FieldMappingRecommendation(
                source_field=source_field,
                candidates=candidates,
                recommended_target=recommended_target,
                recommended_confidence=recommended_confidence
            ))
        
        return recommendations
    
    def _get_history_mappings(
        self,
        source_field: str,
        source_system: str,
        document_type: Optional[str],
        user_id: Optional[str]
    ) -> List[MappingCandidate]:
        """æŸ¥è¯¢å†å²æ˜ å°„"""
        # æ„å»ºæŸ¥è¯¢ï¼ˆä¼˜å…ˆåŒ¹é…ç”¨æˆ·ã€ç³»ç»Ÿã€å•æ®ç±»å‹ï¼‰
        query = self.db.query(FieldMappingHistory).filter(
            FieldMappingHistory.source_field_name == source_field,
            FieldMappingHistory.source_system == source_system,
            FieldMappingHistory.is_confirmed == True,
            FieldMappingHistory.is_rejected == False
        )
        
        if document_type:
            # å…ˆæŸ¥è¯¢åŒ¹é…å•æ®ç±»å‹çš„
            query_type = query.filter(
                FieldMappingHistory.document_type == document_type
            ).order_by(
                FieldMappingHistory.usage_count.desc(),
                FieldMappingHistory.last_used_at.desc()
            ).limit(5).all()
            
            if query_type:
                return [
                    MappingCandidate(
                        target_field=m.target_field_name,
                        confidence=min(0.95 + m.usage_count * 0.01, 1.0),  # ä½¿ç”¨æ¬¡æ•°è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜
                        method='history',
                        source=f'å†å²æ˜ å°„ï¼ˆ{m.usage_count}æ¬¡ä½¿ç”¨ï¼‰'
                    )
                    for m in query_type
                ]
        
        # æŸ¥è¯¢ä¸é™åˆ¶å•æ®ç±»å‹çš„é€šç”¨æ˜ å°„
        query_general = query.filter(
            FieldMappingHistory.document_type.is_(None)
        ).order_by(
            FieldMappingHistory.usage_count.desc(),
            FieldMappingHistory.last_used_at.desc()
        ).limit(3).all()
        
        return [
            MappingCandidate(
                target_field=m.target_field_name,
                confidence=min(0.85 + m.usage_count * 0.01, 0.95),
                method='history',
                source=f'å†å²æ˜ å°„ï¼ˆé€šç”¨ï¼Œ{m.usage_count}æ¬¡ä½¿ç”¨ï¼‰'
            )
            for m in query_general
        ]
    
    def _apply_mapping_rules(
        self,
        source_field: str,
        source_system: str,
        document_type: Optional[str]
    ) -> List[MappingCandidate]:
        """åº”ç”¨æ˜ å°„è§„åˆ™"""
        candidates = []
        
        # æŸ¥è¯¢åŒ¹é…çš„è§„åˆ™
        rules = self.db.query(FieldMappingRule).filter(
            FieldMappingRule.is_active == True
        ).all()
        
        for rule in rules:
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ•°æ®æºå’Œå•æ®ç±»å‹
            if rule.source_system and rule.source_system != source_system:
                continue
            if rule.document_type and rule.document_type != document_type:
                continue
            
            # æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ¹é…è§„åˆ™
            if self._match_pattern(source_field, rule.source_pattern, rule.match_type):
                candidates.append(MappingCandidate(
                    target_field=rule.target_field_name,
                    confidence=0.9,  # è§„åˆ™åŒ¹é…ç½®ä¿¡åº¦è¾ƒé«˜
                    method='rule',
                    source=f'ç³»ç»Ÿè§„åˆ™ï¼š{rule.source_pattern}'
                ))
        
        return candidates
    
    def _calculate_similarity_mappings(
        self,
        source_field: str,
        document_type: Optional[str]
    ) -> List[MappingCandidate]:
        """è®¡ç®—ç›¸ä¼¼åº¦åŒ¹é…"""
        candidates = []
        
        # è·å–æ ‡å‡†å­—æ®µåˆ—è¡¨ï¼ˆæ ¹æ®å•æ®ç±»å‹ï¼‰
        standard_fields = self._get_standard_fields(document_type)
        
        for target_field in standard_fields:
            # è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            similarity = self._calculate_string_similarity(source_field, target_field)
            
            if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                candidates.append(MappingCandidate(
                    target_field=target_field,
                    confidence=similarity,
                    method='similarity',
                    source=f'ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆ{similarity:.2f}ï¼‰'
                ))
        
        return candidates
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        # æ–¹æ³•1ï¼šSequenceMatcherï¼ˆç¼–è¾‘è·ç¦»ï¼‰
        similarity_1 = SequenceMatcher(None, str1.lower(), str2.lower()).ratio()
        
        # æ–¹æ³•2ï¼šåŒ…å«å…³ç³»
        similarity_2 = 0.0
        if str1.lower() in str2.lower() or str2.lower() in str1.lower():
            similarity_2 = 0.7
        
        # æ–¹æ³•3ï¼šå­—ç¬¦äº¤é›†æ¯”ä¾‹
        set1 = set(str1.lower())
        set2 = set(str2.lower())
        if len(set1) > 0 and len(set2) > 0:
            similarity_3 = len(set1 & set2) / len(set1 | set2)
        else:
            similarity_3 = 0.0
        
        # å–æœ€å¤§å€¼
        return max(similarity_1, similarity_2, similarity_3)
    
    def _match_pattern(self, field_name: str, pattern: str, match_type: str) -> bool:
        """åŒ¹é…å­—æ®µæ¨¡å¼"""
        field_lower = field_name.lower()
        pattern_lower = pattern.lower()
        
        if match_type == 'exact':
            return field_lower == pattern_lower
        elif match_type == 'prefix':
            return field_lower.startswith(pattern_lower)
        elif match_type == 'suffix':
            return field_lower.endswith(pattern_lower)
        elif match_type == 'contains':
            return pattern_lower in field_lower
        elif match_type == 'regex':
            import re
            return bool(re.match(pattern, field_name, re.IGNORECASE))
        else:
            return False
    
    def _get_standard_fields(self, document_type: Optional[str]) -> List[str]:
        """è·å–æ ‡å‡†å­—æ®µåˆ—è¡¨"""
        # æ ‡å‡†å­—æ®µåˆ—è¡¨ï¼ˆæ ¹æ®å•æ®ç±»å‹ï¼‰
        base_fields = [
            # å•æ®å¤´å­—æ®µ
            'å•æ®å·', 'document_id', 'document_number',
            'å•æ®æ—¥æœŸ', 'document_date', 'date',
            'å®¢æˆ·åç§°', 'customer_name', 'supplier_name',
            'ç»è¥ä¸»ä½“åç§°', 'business_entity_name',
            'ä¸å«ç¨é‡‘é¢', 'ex_tax_amount', 'amount_excluding_tax',
            'ç¨é¢', 'tax_amount', 'tax',
            'ä»·ç¨åˆè®¡', 'total_amount_with_tax', 'amount_including_tax',
            # æ˜ç»†å­—æ®µ
            'äº§å“åç§°', 'product_name', 'item_name',
            'æ•°é‡', 'quantity', 'qty',
            'å•ä»·', 'unit_price', 'price',
            'è®¡é‡å•ä½', 'unit', 'unit_name',
        ]
        
        # å¯ä»¥æ ¹æ®å•æ®ç±»å‹æ‰©å±•
        if document_type == 'purchase_order':
            base_fields.extend(['ä¾›åº”å•†åç§°', 'supplier_name'])
        elif document_type == 'sales_order':
            base_fields.extend(['å®¢æˆ·åç§°', 'customer_name'])
        
        return base_fields
    
    def _deduplicate_and_sort_candidates(
        self,
        candidates: List[MappingCandidate]
    ) -> List[MappingCandidate]:
        """å»é‡å¹¶æ’åºå€™é€‰"""
        # æŒ‰ç›®æ ‡å­—æ®µå»é‡ï¼ˆä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ï¼‰
        unique_candidates = {}
        for candidate in candidates:
            key = candidate.target_field
            if key not in unique_candidates or candidate.confidence > unique_candidates[key].confidence:
                unique_candidates[key] = candidate
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        return sorted(
            unique_candidates.values(),
            key=lambda x: x.confidence,
            reverse=True
        )
```

### 2. æ˜ å°„è®°å½•å’Œæ›´æ–°

```python
def save_mapping_history(
    self,
    source_field: str,
    target_field: str,
    source_system: str,
    document_type: Optional[str],
    user_id: Optional[str],
    match_method: str,
    confidence: float,
    is_confirmed: bool = True
):
    """ä¿å­˜æ˜ å°„å†å²è®°å½•"""
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = self.db.query(FieldMappingHistory).filter(
        FieldMappingHistory.source_system == source_system,
        FieldMappingHistory.document_type == document_type,
        FieldMappingHistory.source_field_name == source_field,
        FieldMappingHistory.target_field_name == target_field,
        FieldMappingHistory.user_id == user_id
    ).first()
    
    if existing:
        # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
        existing.usage_count += 1
        existing.last_used_at = sa.func.now()
        existing.is_confirmed = is_confirmed
        existing.is_rejected = False
        existing.match_confidence = confidence
        existing.match_method = match_method
    else:
        # åˆ›å»ºæ–°è®°å½•
        new_mapping = FieldMappingHistory(
            source_system=source_system,
            document_type=document_type,
            user_id=user_id,
            source_field_name=source_field,
            target_field_name=target_field,
            match_confidence=confidence,
            match_method=match_method,
            usage_count=1,
            is_confirmed=is_confirmed,
            is_rejected=False
        )
        self.db.add(new_mapping)
    
    self.db.commit()
```

---

## ğŸ”Œ APIç«¯ç‚¹è®¾è®¡

### 1. è·å–æ˜ å°„æ¨è

```http
POST /api/v1/data-import/recommend-mappings
Content-Type: application/json

{
  "source_fields": ["é‡‡è´­è®¢å•å·", "è®¢å•æ—¥æœŸ", "ä¾›åº”å•†", "ç‰©æ–™åç§°", "æ•°é‡", "å•ä»·", "é‡‘é¢"],
  "source_system": "erp_system_a",
  "document_type": "purchase_order",
  "user_id": "uuid"  // å¯é€‰
}
```

**å“åº”**:
```json
{
  "recommendations": [
    {
      "source_field": "é‡‡è´­è®¢å•å·",
      "recommended_target": "å•æ®å·",
      "recommended_confidence": 0.98,
      "method": "history",
      "candidates": [
        {
          "target_field": "å•æ®å·",
          "confidence": 0.98,
          "method": "history",
          "source": "å†å²æ˜ å°„ï¼ˆ15æ¬¡ä½¿ç”¨ï¼‰"
        },
        {
          "target_field": "document_id",
          "confidence": 0.75,
          "method": "similarity",
          "source": "ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆ0.75ï¼‰"
        }
      ]
    },
    {
      "source_field": "è®¢å•æ—¥æœŸ",
      "recommended_target": "å•æ®æ—¥æœŸ",
      "recommended_confidence": 0.95,
      "method": "history",
      "candidates": [...]
    },
    {
      "source_field": "ç‰©æ–™åç§°",
      "recommended_target": "äº§å“åç§°",
      "recommended_confidence": 0.82,
      "method": "similarity",
      "candidates": [...]
    }
  ],
  "summary": {
    "total_fields": 7,
    "high_confidence_count": 5,  // ç½®ä¿¡åº¦ >= 0.9
    "medium_confidence_count": 2,  // 0.7 <= ç½®ä¿¡åº¦ < 0.9
    "low_confidence_count": 0  // ç½®ä¿¡åº¦ < 0.7
  }
}
```

### 2. ç¡®è®¤æ˜ å°„ï¼ˆè®°å½•å†å²ï¼‰

```http
POST /api/v1/data-import/confirm-mappings
Content-Type: application/json

{
  "source_system": "erp_system_a",
  "document_type": "purchase_order",
  "user_id": "uuid",  // å¯é€‰
  "mappings": [
    {
      "source_field": "é‡‡è´­è®¢å•å·",
      "target_field": "å•æ®å·",
      "confidence": 0.98,
      "method": "history",
      "is_confirmed": true
    },
    {
      "source_field": "è®¢å•æ—¥æœŸ",
      "target_field": "å•æ®æ—¥æœŸ",
      "confidence": 0.95,
      "method": "history",
      "is_confirmed": true
    },
    {
      "source_field": "ç‰©æ–™åç§°",
      "target_field": "äº§å“åç§°",
      "confidence": 0.82,
      "method": "similarity",
      "is_confirmed": true
    }
  ]
}
```

**å“åº”**:
```json
{
  "success": true,
  "saved_count": 3,
  "updated_count": 2,
  "new_count": 1
}
```

### 3. æ‹’ç»æ˜ å°„ï¼ˆè®°å½•æ‹’ç»å†å²ï¼‰

```http
POST /api/v1/data-import/reject-mapping
Content-Type: application/json

{
  "source_system": "erp_system_a",
  "document_type": "purchase_order",
  "source_field": "ç‰©æ–™åç§°",
  "target_field": "äº§å“åç§°",
  "reason": "ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´"
}
```

### 4. æŸ¥è¯¢å†å²æ˜ å°„

```http
GET /api/v1/data-import/mapping-history
Query Parameters:
  - source_system: erp_system_a
  - document_type: purchase_order (å¯é€‰)
  - user_id: uuid (å¯é€‰)
  - limit: 50
```

**å“åº”**:
```json
{
  "mappings": [
    {
      "id": "uuid",
      "source_field": "é‡‡è´­è®¢å•å·",
      "target_field": "å•æ®å·",
      "usage_count": 15,
      "last_used_at": "2025-01-20T10:30:00Z",
      "match_method": "history",
      "confidence": 0.98
    },
    ...
  ],
  "total": 25
}
```

---

## ğŸ“ ä½¿ç”¨æµç¨‹ç¤ºä¾‹

### å®Œæ•´å¯¼å…¥æµç¨‹ï¼ˆå¸¦æ™ºèƒ½æ˜ å°„ï¼‰

```python
# 1. ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
file_path = "purchase_order.xlsx"

# 2. ç³»ç»Ÿè§£ææ–‡ä»¶å­—æ®µ
source_fields = extract_fields(file_path)  # ["é‡‡è´­è®¢å•å·", "è®¢å•æ—¥æœŸ", "ä¾›åº”å•†", ...]

# 3. è·å–æ˜ å°„æ¨è
mapper = IntelligentFieldMapper(db_session)
recommendations = mapper.recommend_mappings(
    source_fields=source_fields,
    source_system="erp_system_a",
    document_type="purchase_order",
    user_id="user_123"
)

# 4. å‰ç«¯å±•ç¤ºæ¨èï¼Œç”¨æˆ·ç¡®è®¤æˆ–è°ƒæ•´
# ç”¨æˆ·ç¡®è®¤åçš„æ˜ å°„ï¼š
confirmed_mappings = {
    "é‡‡è´­è®¢å•å·": "å•æ®å·",  # é«˜ç½®ä¿¡åº¦ï¼Œè‡ªåŠ¨åº”ç”¨
    "è®¢å•æ—¥æœŸ": "å•æ®æ—¥æœŸ",   # é«˜ç½®ä¿¡åº¦ï¼Œè‡ªåŠ¨åº”ç”¨
    "ä¾›åº”å•†": "å®¢æˆ·åç§°",     # ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´
    "ç‰©æ–™åç§°": "äº§å“åç§°",   # ä¸­ç½®ä¿¡åº¦ï¼Œç”¨æˆ·ç¡®è®¤
    "æ•°é‡": "æ•°é‡",          # å®Œå…¨åŒ¹é…ï¼Œè‡ªåŠ¨åº”ç”¨
    "å•ä»·": "å•ä»·",          # å®Œå…¨åŒ¹é…ï¼Œè‡ªåŠ¨åº”ç”¨
    "é‡‘é¢": "ä¸å«ç¨é‡‘é¢"     # ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´
}

# 5. ä¿å­˜æ˜ å°„å†å²
for source_field, target_field in confirmed_mappings.items():
    rec = next(r for r in recommendations if r.source_field == source_field)
    mapper.save_mapping_history(
        source_field=source_field,
        target_field=target_field,
        source_system="erp_system_a",
        document_type="purchase_order",
        user_id="user_123",
        match_method=rec.method,
        confidence=rec.recommended_confidence,
        is_confirmed=True
    )

# 6. åº”ç”¨æ˜ å°„ï¼Œç»§ç»­å¤„ç†
processed_data = apply_field_mappings(raw_data, confirmed_mappings)
```

---

## âœ… ç‰¹æ€§æ€»ç»“

### æ ¸å¿ƒåŠŸèƒ½

1. âœ… **å†å²æ˜ å°„ä¼˜å…ˆ**ï¼šä¼˜å…ˆä½¿ç”¨å†å²æ˜ å°„è®°å½•ï¼Œç½®ä¿¡åº¦æ›´é«˜
2. âœ… **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šå­—ç¬¦ä¸²ç›¸ä¼¼åº¦åŒ¹é…ï¼Œæ”¯æŒå¤šç§ç®—æ³•
3. âœ… **è§„åˆ™åŒ¹é…**ï¼šç³»ç»Ÿçº§æ˜ å°„è§„åˆ™ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼
4. âœ… **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šè€ƒè™‘æ•°æ®æºã€å•æ®ç±»å‹ã€ç”¨æˆ·ID
5. âœ… **æŒç»­å­¦ä¹ **ï¼šè®°å½•ç”¨æˆ·ç¡®è®¤ï¼Œæé«˜åç»­å‡†ç¡®æ€§
6. âœ… **ä½¿ç”¨ç»Ÿè®¡**ï¼šè·Ÿè¸ªä½¿ç”¨æ¬¡æ•°ï¼Œä¼˜å…ˆæ¨èå¸¸ç”¨æ˜ å°„

### ä¼˜åŠ¿

- ğŸ¯ **é™ä½æ“ä½œéš¾åº¦**ï¼šè‡ªåŠ¨æ¨èï¼Œå‡å°‘90%çš„æ‰‹åŠ¨æ˜ å°„å·¥ä½œ
- ğŸ§  **æŒç»­å­¦ä¹ **ï¼šç³»ç»Ÿè¶Šç”¨è¶Šå‡†ç¡®
- ğŸ‘¤ **ä¸ªäººåŒ–**ï¼šæ”¯æŒç”¨æˆ·çº§åˆ«çš„ä¸ªäººåŒ–æ˜ å°„
- ğŸ“Š **ç»Ÿè®¡åˆ†æ**ï¼šè·Ÿè¸ªæ˜ å°„ä½¿ç”¨æƒ…å†µ
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ”¯æŒç³»ç»Ÿçº§è§„åˆ™å’Œç”¨æˆ·çº§æ˜ å°„

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å¤æ‚å•æ®æ ¼å¼å¤„ç†](./COMPLEX_DOCUMENT_FORMAT_HANDLING.md)
- [åˆ†å·¥ç­–ç•¥æ–‡æ¡£](./COMPLEX_IMPORT_DIVISION_STRATEGY.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-01-23

