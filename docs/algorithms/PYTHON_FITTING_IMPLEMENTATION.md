# Pythonå†å²æ•°æ®æ‹Ÿåˆå®ç°ç¤ºä¾‹

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å…·ä½“çš„Pythonå®ç°ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨å†å²æ•°æ®æ‹Ÿåˆæ¥ä¼˜åŒ–å…¨é“¾è·¯å¢é‡å…¬å¼ã€‚

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. æ•°æ®é¢„å¤„ç†æ¨¡å—
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

class HistoricalDataPreprocessor:
    """å†å²æ•°æ®é¢„å¤„ç†ç±»"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.minmax_scaler = MinMaxScaler()
        
    def load_historical_data(self, data_path: str) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        try:
            data = pd.read_csv(data_path)
            print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {data.shape}")
            return data
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def detect_outliers(self, data: pd.DataFrame, method: str = 'iqr') -> pd.DataFrame:
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        if method == 'iqr':
            # IQRæ–¹æ³•
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # æ ‡è®°å¼‚å¸¸å€¼
            outliers = ((data < lower_bound) | (data > upper_bound)).any(axis=1)
            print(f"æ£€æµ‹åˆ° {outliers.sum()} ä¸ªå¼‚å¸¸å€¼")
            
            # å¤„ç†å¼‚å¸¸å€¼ï¼ˆç”¨ä¸­ä½æ•°æ›¿æ¢ï¼‰
            data_cleaned = data.copy()
            for col in data.columns:
                if outliers[col].any():
                    median_val = data[col].median()
                    data_cleaned.loc[outliers[col], col] = median_val
                    
            return data_cleaned
            
        elif method == 'zscore':
            # Z-scoreæ–¹æ³•
            z_scores = np.abs((data - data.mean()) / data.std())
            outliers = (z_scores > 3).any(axis=1)
            print(f"æ£€æµ‹åˆ° {outliers.sum()} ä¸ªå¼‚å¸¸å€¼")
            
            # å¤„ç†å¼‚å¸¸å€¼
            data_cleaned = data.copy()
            for col in data.columns:
                if outliers[col].any():
                    median_val = data[col].median()
                    data_cleaned.loc[outliers[col], col] = median_val
                    
            return data_cleaned
    
    def fill_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç¼ºå¤±å€¼å¡«å……"""
        # ä½¿ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å……
        data_filled = data.fillna(method='ffill').fillna(method='bfill')
        
        # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨ä¸­ä½æ•°å¡«å……
        for col in data_filled.columns:
            if data_filled[col].isnull().any():
                data_filled[col].fillna(data_filled[col].median(), inplace=True)
                
        return data_filled
    
    def normalize_data(self, data: pd.DataFrame, method: str = 'standard') -> pd.DataFrame:
        """æ•°æ®æ ‡å‡†åŒ–"""
        if method == 'standard':
            return pd.DataFrame(
                self.scaler.fit_transform(data),
                columns=data.columns,
                index=data.index
            )
        elif method == 'minmax':
            return pd.DataFrame(
                self.minmax_scaler.fit_transform(data),
                columns=data.columns,
                index=data.index
            )
        else:
            return data
    
    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹"""
        print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # 1. å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
        data_cleaned = self.detect_outliers(data, method='iqr')
        
        # 2. ç¼ºå¤±å€¼å¡«å……
        data_filled = self.fill_missing_values(data_cleaned)
        
        # 3. æ•°æ®æ ‡å‡†åŒ–
        data_normalized = self.normalize_data(data_filled, method='standard')
        
        print("æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return data_normalized
```

### 2. æ¨¡å‹æ‹Ÿåˆæ¨¡å—
```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, GridSearchCV
import xgboost as xgb
import lightgbm as lgb

class ModelFitter:
    """æ¨¡å‹æ‹Ÿåˆç±»"""
    
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.best_score = 0
        
    def fit_linear_models(self, X: np.ndarray, y: np.ndarray) -> dict:
        """æ‹Ÿåˆçº¿æ€§æ¨¡å‹"""
        models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=1.0)
        }
        
        results = {}
        for name, model in models.items():
            model.fit(X, y)
            y_pred = model.predict(X)
            
            results[name] = {
                'model': model,
                'r2': r2_score(y, y_pred),
                'rmse': np.sqrt(mean_squared_error(y, y_pred)),
                'mae': mean_absolute_error(y, y_pred)
            }
            
        return results
    
    def fit_ensemble_models(self, X: np.ndarray, y: np.ndarray) -> dict:
        """æ‹Ÿåˆé›†æˆæ¨¡å‹"""
        models = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'xgboost': xgb.XGBRegressor(n_estimators=100, random_state=42),
            'lightgbm': lgb.LGBMRegressor(n_estimators=100, random_state=42)
        }
        
        results = {}
        for name, model in models.items():
            model.fit(X, y)
            y_pred = model.predict(X)
            
            results[name] = {
                'model': model,
                'r2': r2_score(y, y_pred),
                'rmse': np.sqrt(mean_squared_error(y, y_pred)),
                'mae': mean_absolute_error(y, y_pred)
            }
            
        return results
    
    def fit_neural_network(self, X: np.ndarray, y: np.ndarray) -> dict:
        """æ‹Ÿåˆç¥ç»ç½‘ç»œæ¨¡å‹"""
        model = MLPRegressor(
            hidden_layer_sizes=(100, 50),
            activation='relu',
            solver='adam',
            max_iter=1000,
            random_state=42
        )
        
        model.fit(X, y)
        y_pred = model.predict(X)
        
        return {
            'model': model,
            'r2': r2_score(y, y_pred),
            'rmse': np.sqrt(mean_squared_error(y, y_pred)),
            'mae': mean_absolute_error(y, y_pred)
        }
    
    def hyperparameter_tuning(self, X: np.ndarray, y: np.ndarray, model_name: str) -> dict:
        """è¶…å‚æ•°è°ƒä¼˜"""
        if model_name == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            }
            model = RandomForestRegressor(random_state=42)
            
        elif model_name == 'xgboost':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2]
            }
            model = xgb.XGBRegressor(random_state=42)
            
        else:
            return None
        
        grid_search = GridSearchCV(
            model, param_grid, cv=5, scoring='r2', n_jobs=-1
        )
        grid_search.fit(X, y)
        
        return {
            'model': grid_search.best_estimator_,
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_
        }
    
    def select_best_model(self, all_results: dict) -> dict:
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        best_model_name = None
        best_r2 = 0
        
        for model_name, results in all_results.items():
            if results['r2'] > best_r2:
                best_r2 = results['r2']
                best_model_name = model_name
        
        return {
            'model_name': best_model_name,
            'model': all_results[best_model_name]['model'],
            'r2': best_r2,
            'rmse': all_results[best_model_name]['rmse'],
            'mae': all_results[best_model_name]['mae']
        }
```

### 3. æ—¶é—´åºåˆ—åˆ†ææ¨¡å—
```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.vector_ar.var_model import VAR
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
import matplotlib.pyplot as plt
import seaborn as sns

class TimeSeriesAnalyzer:
    """æ—¶é—´åºåˆ—åˆ†æç±»"""
    
    def __init__(self):
        self.arima_model = None
        self.var_model = None
        
    def check_stationarity(self, series: pd.Series) -> dict:
        """æ£€æŸ¥æ—¶é—´åºåˆ—å¹³ç¨³æ€§"""
        result = adfuller(series)
        
        return {
            'adf_statistic': result[0],
            'p_value': result[1],
            'critical_values': result[4],
            'is_stationary': result[1] < 0.05
        }
    
    def fit_arima_model(self, series: pd.Series, order: tuple = (1, 1, 1)) -> dict:
        """æ‹ŸåˆARIMAæ¨¡å‹"""
        try:
            model = ARIMA(series, order=order)
            fitted_model = model.fit()
            
            return {
                'model': fitted_model,
                'aic': fitted_model.aic,
                'bic': fitted_model.bic,
                'summary': fitted_model.summary()
            }
        except Exception as e:
            print(f"ARIMAæ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            return None
    
    def fit_var_model(self, data: pd.DataFrame, maxlags: int = 5) -> dict:
        """æ‹ŸåˆVARæ¨¡å‹"""
        try:
            model = VAR(data)
            fitted_model = model.fit(maxlags=maxlags)
            
            return {
                'model': fitted_model,
                'aic': fitted_model.aic,
                'bic': fitted_model.bic,
                'summary': fitted_model.summary()
            }
        except Exception as e:
            print(f"VARæ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            return None
    
    def granger_causality_test(self, data: pd.DataFrame, maxlag: int = 4) -> dict:
        """æ ¼å…°æ°å› æœæ£€éªŒ"""
        results = {}
        
        for col1 in data.columns:
            for col2 in data.columns:
                if col1 != col2:
                    try:
                        test_result = grangercausalitytests(
                            data[[col1, col2]], maxlag=maxlag, verbose=False
                        )
                        results[f"{col1}_to_{col2}"] = test_result
                    except Exception as e:
                        print(f"æ ¼å…°æ°å› æœæ£€éªŒå¤±è´¥ {col1} -> {col2}: {e}")
        
        return results
```

### 4. ååŒæ•ˆåº”åˆ†ææ¨¡å—
```python
class SynergyEffectAnalyzer:
    """ååŒæ•ˆåº”åˆ†æç±»"""
    
    def __init__(self):
        self.synergy_models = {}
        
    def calculate_interaction_terms(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—äº¤äº’é¡¹"""
        interaction_data = data.copy()
        
        # è®¡ç®—ä¸¤ä¸¤äº¤äº’é¡¹
        for col1 in data.columns:
            for col2 in data.columns:
                if col1 != col2:
                    interaction_col = f"{col1}_x_{col2}"
                    interaction_data[interaction_col] = data[col1] * data[col2]
        
        return interaction_data
    
    def analyze_synergy_effects(self, X: np.ndarray, y: np.ndarray) -> dict:
        """åˆ†æååŒæ•ˆåº”"""
        # 1. åŸºç¡€æ¨¡å‹ï¼ˆæ— äº¤äº’é¡¹ï¼‰
        base_model = LinearRegression()
        base_model.fit(X, y)
        base_r2 = base_model.score(X, y)
        
        # 2. äº¤äº’é¡¹æ¨¡å‹
        interaction_terms = self.calculate_interaction_terms(pd.DataFrame(X))
        interaction_model = LinearRegression()
        interaction_model.fit(interaction_terms, y)
        interaction_r2 = interaction_model.score(interaction_terms, y)
        
        # 3. ååŒæ•ˆåº”å¼ºåº¦
        synergy_strength = interaction_r2 - base_r2
        
        return {
            'base_r2': base_r2,
            'interaction_r2': interaction_r2,
            'synergy_strength': synergy_strength,
            'base_model': base_model,
            'interaction_model': interaction_model
        }
    
    def identify_synergy_thresholds(self, data: pd.DataFrame, target: str) -> dict:
        """è¯†åˆ«ååŒæ•ˆåº”é˜ˆå€¼"""
        thresholds = {}
        
        for col1 in data.columns:
            if col1 != target:
                for col2 in data.columns:
                    if col2 != target and col2 != col1:
                        # è®¡ç®—ååŒæ•ˆåº”é˜ˆå€¼
                        synergy_data = data[[col1, col2, target]].copy()
                        synergy_data[f"{col1}_x_{col2}"] = synergy_data[col1] * synergy_data[col2]
                        
                        # åˆ†æ®µå›å½’åˆ†æ
                        threshold = self.find_synergy_threshold(synergy_data, col1, col2, target)
                        thresholds[f"{col1}_x_{col2}"] = threshold
        
        return thresholds
    
    def find_synergy_threshold(self, data: pd.DataFrame, col1: str, col2: str, target: str) -> float:
        """å¯»æ‰¾ååŒæ•ˆåº”é˜ˆå€¼"""
        # ç®€åŒ–çš„é˜ˆå€¼å¯»æ‰¾æ–¹æ³•
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        interaction_col = f"{col1}_x_{col2}"
        
        # è®¡ç®—äº¤äº’é¡¹çš„ç»Ÿè®¡ç‰¹å¾
        interaction_mean = data[interaction_col].mean()
        interaction_std = data[interaction_col].std()
        
        # é˜ˆå€¼è®¾ä¸ºå‡å€¼åŠ ä¸€ä¸ªæ ‡å‡†å·®
        threshold = interaction_mean + interaction_std
        
        return threshold
```

### 5. é¢„æµ‹ä¸ä¼˜åŒ–æ¨¡å—
```python
class EnhancedPredictor:
    """å¢å¼ºç‰ˆé¢„æµ‹å™¨"""
    
    def __init__(self, fitted_models: dict):
        self.models = fitted_models
        self.best_model = None
        
    def predict_with_confidence(self, X: np.ndarray, model_name: str = None) -> dict:
        """å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹"""
        if model_name is None:
            model_name = self.get_best_model_name()
        
        model = self.models[model_name]['model']
        predictions = model.predict(X)
        
        # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        confidence = self.calculate_prediction_confidence(X, model)
        
        return {
            'predictions': predictions,
            'confidence': confidence,
            'model_name': model_name
        }
    
    def calculate_prediction_confidence(self, X: np.ndarray, model) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        # ç®€åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—æ–¹æ³•
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•
        try:
            # ä½¿ç”¨æ¨¡å‹çš„scoreæ–¹æ³•ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡
            if hasattr(model, 'score'):
                return model.score(X, X)  # è¿™é‡Œéœ€è¦å®é™…çš„yå€¼
            else:
                return 0.8  # é»˜è®¤ç½®ä¿¡åº¦
        except:
            return 0.8
    
    def get_best_model_name(self) -> str:
        """è·å–æœ€ä½³æ¨¡å‹åç§°"""
        best_r2 = 0
        best_model_name = None
        
        for model_name, results in self.models.items():
            if results['r2'] > best_r2:
                best_r2 = results['r2']
                best_model_name = model_name
        
        return best_model_name
    
    def generate_optimization_recommendations(self, current_state: dict) -> dict:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = {
            'asset_optimization': [],
            'capability_optimization': [],
            'synergy_optimization': []
        }
        
        # åŸºäºå½“å‰çŠ¶æ€ç”Ÿæˆä¼˜åŒ–å»ºè®®
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ä¸šåŠ¡é€»è¾‘æ¥å®ç°
        
        return recommendations
```

### 6. ä¸»ç¨‹åºç¤ºä¾‹
```python
def main():
    """ä¸»ç¨‹åºç¤ºä¾‹"""
    print("å¼€å§‹å†å²æ•°æ®æ‹Ÿåˆä¼˜åŒ–...")
    
    # 1. æ•°æ®é¢„å¤„ç†
    preprocessor = HistoricalDataPreprocessor()
    data = preprocessor.load_historical_data('historical_data.csv')
    
    if data is None:
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    processed_data = preprocessor.preprocess_data(data)
    
    # 2. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
    feature_columns = ['design_capability', 'design_asset', 'production_capability', 'production_asset']
    target_column = 'product_efficiency'
    
    X = processed_data[feature_columns].values
    y = processed_data[target_column].values
    
    # 3. æ¨¡å‹æ‹Ÿåˆ
    fitter = ModelFitter()
    
    # æ‹Ÿåˆçº¿æ€§æ¨¡å‹
    linear_results = fitter.fit_linear_models(X, y)
    
    # æ‹Ÿåˆé›†æˆæ¨¡å‹
    ensemble_results = fitter.fit_ensemble_models(X, y)
    
    # æ‹Ÿåˆç¥ç»ç½‘ç»œ
    nn_results = fitter.fit_neural_network(X, y)
    
    # 4. æ¨¡å‹é€‰æ‹©
    all_results = {**linear_results, **ensemble_results, 'neural_network': nn_results}
    best_model = fitter.select_best_model(all_results)
    
    print(f"æœ€ä½³æ¨¡å‹: {best_model['model_name']}")
    print(f"RÂ²: {best_model['r2']:.4f}")
    print(f"RMSE: {best_model['rmse']:.4f}")
    print(f"MAE: {best_model['mae']:.4f}")
    
    # 5. ååŒæ•ˆåº”åˆ†æ
    synergy_analyzer = SynergyEffectAnalyzer()
    synergy_results = synergy_analyzer.analyze_synergy_effects(X, y)
    
    print(f"ååŒæ•ˆåº”å¼ºåº¦: {synergy_results['synergy_strength']:.4f}")
    
    # 6. æ—¶é—´åºåˆ—åˆ†æ
    ts_analyzer = TimeSeriesAnalyzer()
    stationarity_results = ts_analyzer.check_stationarity(pd.Series(y))
    
    print(f"æ—¶é—´åºåˆ—å¹³ç¨³æ€§: {stationarity_results['is_stationary']}")
    
    # 7. é¢„æµ‹ä¸ä¼˜åŒ–
    predictor = EnhancedPredictor(all_results)
    predictions = predictor.predict_with_confidence(X)
    
    print(f"é¢„æµ‹ç½®ä¿¡åº¦: {predictions['confidence']:.4f}")
    
    print("å†å²æ•°æ®æ‹Ÿåˆä¼˜åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### 1. æ•°æ®æ ¼å¼è¦æ±‚
```csv
# historical_data.csv
date,design_capability,design_asset,production_capability,production_asset,product_efficiency
2023-01-01,0.8,1.2,0.9,1.1,0.75
2023-02-01,0.85,1.3,0.95,1.15,0.78
...
```

### 2. è¿è¡Œç¤ºä¾‹
```bash
# å®‰è£…ä¾èµ–
pip install pandas numpy scikit-learn xgboost lightgbm statsmodels matplotlib seaborn

# è¿è¡Œç¨‹åº
python historical_data_fitting.py
```

### 3. è¾“å‡ºç»“æœ
```
å¼€å§‹å†å²æ•°æ®æ‹Ÿåˆä¼˜åŒ–...
æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: (1000, 6)
æ£€æµ‹åˆ° 15 ä¸ªå¼‚å¸¸å€¼
æ•°æ®é¢„å¤„ç†å®Œæˆ
æœ€ä½³æ¨¡å‹: xgboost
RÂ²: 0.8542
RMSE: 0.0823
MAE: 0.0654
ååŒæ•ˆåº”å¼ºåº¦: 0.1234
æ—¶é—´åºåˆ—å¹³ç¨³æ€§: True
é¢„æµ‹ç½®ä¿¡åº¦: 0.8765
å†å²æ•°æ®æ‹Ÿåˆä¼˜åŒ–å®Œæˆï¼
```

## ğŸ¯ æ€»ç»“

é€šè¿‡Pythonå®ç°çš„å†å²æ•°æ®æ‹Ÿåˆä¼˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

1. **æå‡é¢„æµ‹å‡†ç¡®æ€§**ï¼šä»ç®€å•çº¿æ€§å…³ç³»å‡çº§ä¸ºå¤æ‚çš„éçº¿æ€§æ¨¡å‹
2. **è¯†åˆ«éšè—å…³ç³»**ï¼šå‘ç°ååŒæ•ˆåº”ã€é˜ˆå€¼æ•ˆåº”ç­‰å¤æ‚å…³ç³»
3. **æä¾›ç§‘å­¦å†³ç­–æ”¯æŒ**ï¼šåŸºäºå†å²æ•°æ®çš„ç§‘å­¦é¢„æµ‹å’Œä¼˜åŒ–å»ºè®®
4. **æ”¯æŒå®æ—¶ä¼˜åŒ–**ï¼šåŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œé€‚åº”ä¸šåŠ¡å˜åŒ–

è¿™ç§æ–¹æ³•å°†å¤§å¹…æå‡å…¨é“¾è·¯å¢é‡å…¬å¼çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ï¼ğŸš€




